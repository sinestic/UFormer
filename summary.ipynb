{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UFormer: A General U-Shaped Transformer for Image Restoration #\n",
    "## Project summary ##"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Introduction ###\n",
    "The general idea of the UFormer architecture is based on U-Net but with transformers in place of the residual blocks. \n",
    "In Uformer, there are two core designs. First, the novel locally-enhanced window (LeWin) Transformer block, which performs non overlapping window-based self-attention instead of global self-attention. It significantly reduces the computational complexity on high resolution feature map while capturing local context.\n",
    "\n",
    "Second, a learnable multi-scale restoration modulator in the form of a multi-scale spatial bias to adjust features in multiple layers of the Uformer decoder. The modulator demonstrates superior capability for restoring details for various image restoration tasks while introducing marginal extra parameters and computational cost. \n",
    "\n",
    "Powered by these two designs, Uformer enjoys a high capability for capturing both local and global dependencies for image restoration.\n",
    "\n",
    "### 2. Method ###\n",
    "The model that is presented in this project is trained only to do \"deblur\" task of an input image.\n",
    "\n",
    "The method implemented follows the instructions founded in the paper. The input image pass into the first convolutional layer to extract the first feature and resize the number of channel into the number of embeddings. \n",
    "\n",
    "The core of the LeWin block is the Leff block that is composed by the multi head attention layer and an inverse bottleneck wiht a GeLU activation. Before and after the multihead attention layer there are the normalization layer as described in the recent [article](https://arxiv.org/pdf/2002.04745.pdf). \n",
    "The multihead attention layer used to build this architecture is the one implemented inside the Torch library.\n",
    "\n",
    "The LeWin block is after the patching function, that divide the feature maps in windows of 16x16 patch size and followed by a depatch function that undo the patching process, as explained in the article. \n",
    "The encoder block has inside a number of LeWin blocks that depends on the hardware used to train the model, in my case are different from the article, but the number of encoder blocks are the same.\n",
    "\n",
    "Given the architecture shape, the number of decoder blocks is the same as the encoder's number. \n",
    "Each encoder block is followed b a downsample convolutional layer 4x4 with stride 2 and each decoder layer is followed by a transpose convolutional layer 4x4 and stride 2. \n",
    "\n",
    "Last layer is a basic convolutional 3x3 layer that generate the float32 image used to calculate the loss from the target. The loss function is the Charbonnier loss, the details of this loss function are explained [here](https://arxiv.org/pdf/1701.03077.pdf), as explained in the article.\n",
    "\n",
    "The metric used to evaluate the model is the PSNR (Peak signal noise ratio).\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. A little guide to do inference ###\n",
    "To use the model do the following the passages:\n",
    "- install the requirements\n",
    "- download the model weights file\n",
    "- run the ```python3 test.py --model <path_of_weights> --input <dir_that_contains_input_images>```  \n",
    "\n",
    "that's it.\n",
    "\n",
    "or use this notebook that is ready to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#system imports\n",
    "import glob\n",
    "# third party imports\n",
    "import torch\n",
    "# personal imports\n",
    "from model import UFormer\n",
    "from test import test\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "#default path where the images are\n",
    "img_filenames = glob.glob(\"test/*png\")\n",
    "# default path of the model's weight\n",
    "ckpt_path = \"checkpoints/best_model.pt\"\n",
    "torch.cuda.empty_cache()\n",
    "model = UFormer()\n",
    "model.load_state_dict(torch.load(ckpt_path),strict=False)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "# test for each file in img_filenames\n",
    "for fn in img_filenames:\n",
    "    test(model,fn)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Results of evaluation ###\n",
    "The results of the trained model are influenced from the hardware limitation. \n",
    "The graphs describe the PSNR and the validation loss during the training phase\n",
    "![PSNR](.docs/PSNR.png)\n",
    "![Valid](.docs/valid_loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
